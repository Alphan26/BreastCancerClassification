{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbG69QSD28qR"
   },
   "source": [
    "# Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wsrtTwy28qS"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==2.0+cu117 --user -f https://download.pytorch.org/whl/cu117/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yAZXXlHp28qS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import cv2\n",
    "import sys\n",
    "from torchvision import models,transforms,datasets\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import torchvision\n",
    "from sklearn .metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJtcehDm4H4Z",
    "outputId": "64b5758f-3074-4d9a-9f75-50ffc024f9d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A4000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available()) # this should return true\n",
    "torch.cuda.get_device_name() # this should return your graphics card name. Ex) 'NVIDIA RTX A4000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hV1jmdLT5LKF"
   },
   "outputs": [],
   "source": [
    "# # importing the zipfile module \n",
    "\n",
    "\n",
    "# from zipfile import ZipFile \n",
    "\n",
    "#  # loading the temp.zip and creating a zip object \n",
    "# with ZipFile(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis_ddsm.zip\", 'r') as zObject: \n",
    "#     zObject.extractall() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.248386...\n",
       "2     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.267213...\n",
       "11    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.210396...\n",
       "12    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.749566...\n",
       "15    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.987658...\n",
       "Name: image_path, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dicom_data = pd.read_csv(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\dicom_info.csv\")\n",
    "\n",
    "full_mammogram_images = dicom_data[dicom_data.SeriesDescription == 'full mammogram images'].image_path\n",
    "full_mammogram_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mass dataset\n",
    "mass_train = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_train_set.csv')\n",
    "mass_test = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_test_set.csv')\n",
    "\n",
    "#mass_train.head()\n",
    "mass_train.iloc[0][11].split(\"/\")[2]\n",
    "\n",
    "# fix image paths\n",
    "def fix_image_path(data):\n",
    " #   correct dicom paths to correct image paths\n",
    "  for index, img in enumerate(data.values):\n",
    "    img_name = img[11].split(\"/\")[2]\n",
    "    new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\jpeg\\\\\" + img_name\n",
    "    print(new_path)\n",
    "    data.iloc[index,11] = new_path\n",
    "      \n",
    "# apply to datasets\n",
    "fix_image_path(mass_train)\n",
    "fix_image_path(mass_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calc_train = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_train_set.csv')\n",
    "calc_test = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_test_set.csv')\n",
    "\n",
    "fix_image_path(calc_train)\n",
    "fix_image_path(calc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "544\n",
      "1546\n"
     ]
    }
   ],
   "source": [
    "temp_calc_train = calc_train[[\"image file path\",\"pathology\",\"abnormality id\"]]\n",
    "calc_normal = []\n",
    "calc_malignant = []\n",
    "for row,col in temp_calc_train.iterrows():\n",
    "    if col[\"pathology\"] == \"MALIGNANT\":\n",
    "        calc_malignant.append(col[\"image file path\"])\n",
    "    else:\n",
    "        calc_normal.append(col[\"image file path\"])\n",
    "\n",
    "print(len(calc_normal))\n",
    "print(len(calc_malignant))\n",
    "print(len(temp_calc_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n",
      "637\n",
      "1318\n"
     ]
    }
   ],
   "source": [
    "new_df = mass_train[[\"image file path\",\"pathology\",\"abnormality id\"]]\n",
    "\n",
    "normal_path = []\n",
    "malignant_path = []\n",
    "for row,col in new_df.iterrows():\n",
    "    if col[\"pathology\"] == \"MALIGNANT\":\n",
    "        # put this file to the malignant folder\n",
    "        malignant_path.append(col[\"image file path\"])\n",
    "    else:\n",
    "        normal_path.append(col[\"image file path\"])\n",
    "\n",
    "print(len(normal_path))\n",
    "print(len(malignant_path))\n",
    "print(len(new_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\\"\\nprint(len(os.listdir(os.path.join(path,\"cbis-test\\\\malignant\"))))\\nprint(len(os.listdir(os.path.join(path,\"cbis-test\\\\normal\"))))\\nprint(len(os.listdir(os.path.join(path,\"cbis-train\\\\malignant\"))))\\nprint(len(os.listdir(os.path.join(path,\"cbis-train\\\\normal\"))))\\nprint(len(os.listdir(os.path.join(path,\"cbis-val\\\\malignant\"))))\\nprint(len(os.listdir(os.path.join(path,\"cbis-val\\\\normal\"))))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\\"\n",
    "print(len(os.listdir(os.path.join(path,\"cbis-test\\\\malignant\"))))\n",
    "print(len(os.listdir(os.path.join(path,\"cbis-test\\\\normal\"))))\n",
    "print(len(os.listdir(os.path.join(path,\"cbis-train\\\\malignant\"))))\n",
    "print(len(os.listdir(os.path.join(path,\"cbis-train\\\\normal\"))))\n",
    "print(len(os.listdir(os.path.join(path,\"cbis-val\\\\malignant\"))))\n",
    "print(len(os.listdir(os.path.join(path,\"cbis-val\\\\normal\"))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_df = pd.concat([temp_calc_train,new_df])\n",
    "calc_test = calc_test[[\"image file path\",\"pathology\",\"abnormality id\"]]\n",
    "mass_test = mass_test[[\"image file path\",\"pathology\",\"abnormality id\"]]\n",
    "new_test = pd.concat([calc_test,mass_test])\n",
    "\n",
    "#last_df.drop_duplicates(subset=[\"image file path\"],inplace = True)\n",
    "#new_test.drop_duplicates(subset=[\"image file path\"],inplace=True)\n",
    "\n",
    "train = last_df[:2160]\n",
    "val = last_df[2160:]\n",
    "test_df = new_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\ae423\\OneDrive - University of Sussex\\Desktop\\AlphanElmasDissertation\\jpeg\\1.3.6.1.4.1.9590.100.1.2.47414316010368386519740343172775938548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\jpeg\\\\1.3.6.1.4.1.9590.100.1.2.47414316010368386519740343172775938548\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# p1 = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\jpeg\\\\1.3.6.1.4.1.9590.100.1.2.406725628213826290127343763811145520834\\\\1-192.jpg'\n",
    "# p2 = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\a\\\\b\"\n",
    "# p3 = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\a\\\\c\"\n",
    "# shutil.copy(p1,p2)\n",
    "# shutil.copy(p1,p3)\n",
    "# # ayni file li ayni foldera iki kere kaydetmiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res= []\n",
    "# import shutil\n",
    "\n",
    "# def write_into_train_folder():\n",
    "#     for row,col in train.iterrows():\n",
    "        \n",
    "#         for img_name in os.listdir(col[\"image file path\"]):\n",
    "#             img_path = col[\"image file path\"] + \"\\\\\" + img_name\n",
    "#             if col[\"pathology\"] == \"MALIGNANT\":\n",
    "#                 # put this file to the malignant folder\n",
    "#                 new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\malignant\\\\\" + str(col[\"abnormality id\"]) + \"-\" + col[\"image file path\"].split(\"\\\\\")[-1] + \"-\" + img_name\n",
    "#                 #res.append(new_path)\n",
    "#                 shutil.copy(img_path,new_path)\n",
    "#             else:\n",
    "#                 new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\normal\\\\\" + str(col[\"abnormality id\"]) + \"-\" + col[\"image file path\"].split(\"\\\\\")[-1] + \"-\" + img_name\n",
    "#                 #res.append(new_path)\n",
    "#                 shutil.copy(img_path,new_path)\n",
    "#                 # put it into normal folder\n",
    "\n",
    "\n",
    "# def write_into_val_folder():\n",
    "#     for row,col in val.iterrows():\n",
    "#         #res.append(os.listdir(col[\"image file path\"]))\n",
    "#         for img_name in os.listdir(col[\"image file path\"]):\n",
    "#             img_path = col[\"image file path\"] + \"\\\\\" + img_name\n",
    "#             if col[\"pathology\"] == \"MALIGNANT\":\n",
    "#                 # put this file to the malignant folder\n",
    "#                 new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\malignant\\\\\" + str(col[\"abnormality id\"]) + \"-\" + col[\"image file path\"].split(\"\\\\\")[-1] + \"-\" + img_name\n",
    "#                 #res.append(new_path)\n",
    "#                 shutil.copy(img_path,new_path)\n",
    "#             else:\n",
    "#                 new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\normal\\\\\" + str(col[\"abnormality id\"]) + \"-\" + col[\"image file path\"].split(\"\\\\\")[-1] + \"-\" + img_name\n",
    "#                 #res.append(new_path)\n",
    "#                 shutil.copy(img_path,new_path)\n",
    "#                 # put it into normal folder\n",
    "\n",
    "\n",
    "# def write_into_test_folder():\n",
    "#     for row,col in test_df.iterrows():\n",
    "#         #res.append(os.listdir(col[\"image file path\"]))\n",
    "#         for img_name in os.listdir(col[\"image file path\"]):\n",
    "#             img_path = col[\"image file path\"] + \"\\\\\" + img_name\n",
    "#             if col[\"pathology\"] == \"MALIGNANT\":\n",
    "#                 # put this file to the malignant folder\n",
    "#                 new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\malignant\\\\\" + str(col[\"abnormality id\"]) + \"-\" + col[\"image file path\"].split(\"\\\\\")[-1] + \"-\" + img_name\n",
    "#                 #res.append(new_path)\n",
    "#                 shutil.copy(img_path,new_path)\n",
    "#             else:\n",
    "#                 new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\normal\\\\\" + str(col[\"abnormality id\"]) + \"-\" + col[\"image file path\"].split(\"\\\\\")[-1] + \"-\" + img_name\n",
    "#                 #res.append(new_path)\n",
    "#                 shutil.copy(img_path,new_path)\n",
    "                \n",
    "\n",
    "# write_into_train_folder()\n",
    "# write_into_val_folder()\n",
    "# write_into_test_folder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "704\n",
      "704\n",
      "2160\n",
      "704\n",
      "704\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test_df))\n",
    "print(len(val))\n",
    "print(len(os.listdir(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\malignant\")) + len(os.listdir(\"C:\\\\Users\\\\ae423\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\normal\")))\n",
    "print(len(os.listdir(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\malignant\")) + len(os.listdir(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\normal\")))\n",
    "print(len(os.listdir(\"C:\\\\Users\\\\ae423\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\malignant\")) + len(os.listdir(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\normal\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained = True)\n",
    "\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in list(resnet50.parameters())[-4:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malphanhilal\u001b[0m (\u001b[33malphanhilal-university-of-sussex\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2045092a8a4cb295c2895f7e82a8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01127777777777131, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ae423\\OneDrive - University of Sussex\\Desktop\\AlphanElmasDissertation\\wandb\\run-20240911_163505-1i8o6x14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201/runs/1i8o6x14' target=\"_blank\">deep-cloud-6</a></strong> to <a href='https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201' target=\"_blank\">https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201/runs/1i8o6x14' target=\"_blank\">https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201/runs/1i8o6x14</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"Pretrained Output 1\",\n",
    "    # Track hyperparameters and run metadata\n",
    ")\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHE():\n",
    "    \n",
    "    def __init__(self, clipLimit=2.0, tileGridSize=(16,16)):\n",
    "        super().__init__\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "\n",
    "    def __call__(self,sample):\n",
    "      img = np.array(sample)\n",
    "      img = np.moveaxis(img,0,2)\n",
    "      imgray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "      imgray = imgray.astype(np.uint16)\n",
    "      clahe = cv2.createCLAHE(clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
    "      new_img = clahe.apply(imgray)\n",
    "      new_img = cv2.cvtColor(new_img, cv2.COLOR_GRAY2BGR)\n",
    "      new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
    "      new_img = new_img.astype(np.float32)\n",
    "      new_img = np.moveaxis(new_img,2,0)\n",
    "      tensor = torch.tensor(new_img)\n",
    "      #print(img.shape)\n",
    "      return tensor\n",
    "\n",
    "\n",
    "class GaussianNoise():\n",
    "\n",
    "    def __init__(self,mean=0,stddev=0.01):\n",
    "        super().__init__\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self,sample):\n",
    "        sample = sample.cpu()\n",
    "        input_array = sample.data.numpy()\n",
    "\n",
    "        noise = np.random.normal(loc=self.mean, scale=self.stddev, size=np.shape(input_array))\n",
    "\n",
    "        out = np.add(input_array, noise)\n",
    "\n",
    "        output_tensor = torch.from_numpy(out)\n",
    "        out_tensor = Variable(output_tensor)\n",
    "        out = out_tensor.cuda()\n",
    "        out = out.float()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(batch_size):\n",
    "   \n",
    "    transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # data augmentation\n",
    "    transforms.RandomRotation(degrees=(0,180)),\n",
    "    transforms.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # normalization\n",
    "    CLAHE(),\n",
    "    GaussianNoise()\n",
    "\n",
    "])\n",
    "    indices = list(range(192))\n",
    "\n",
    "    train_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train'\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transforms_train)\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "    test_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test'\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transforms_train)\n",
    "    test_subset = torch.utils.data.Subset(test_dataset, indices)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "    val_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val'\n",
    "    val_dataset = datasets.ImageFolder(val_dir, transforms_train)\n",
    "    val_subset = torch.utils.data.Subset(val_dataset, indices)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "    return train_dataloader,test_dataloader,val_dataloader\n",
    "\n",
    "\n",
    "def build_resnet():\n",
    "    model_ft = models.resnet50(weights='IMAGENET1K_V1')\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_ft.fc.in_features # 2048\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 16) # 16 values will be come for the merged model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return model_ft.to(device)\n",
    "\n",
    "      \n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "numerical_predictions = []\n",
    "def pathology_encoder(val):\n",
    "    return \"MALIGNANT\" if val == \"MALIGNANT\" else \"NORMAL\"\n",
    "\n",
    "calc_df = pd.read_csv(\"csv\\calc_case_description_train_set.csv\")\n",
    "c_test = pd.read_csv(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_test_set.csv\")\n",
    "calc_df[\"pathology\"] = calc_df[\"pathology\"].map(pathology_encoder)\n",
    "cols_to_be_used = [\"breast density\",\"left or right breast\",\"image view\",\"abnormality id\",\"calc type\",\"calc distribution\",\"assessment\",\"pathology\"]\n",
    "new_calc_df = calc_df[cols_to_be_used]\n",
    "\n",
    "mass_df = pd.read_csv(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_train_set.csv\")\n",
    "m_test = pd.read_csv(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_test_set.csv\")\n",
    "mass_df[\"pathology\"] = mass_df[\"pathology\"].map(pathology_encoder)\n",
    "cols_to_be_used = [\"breast_density\",\"left or right breast\",\"image view\",\"abnormality id\",\"mass shape\",\"mass margins\",\"assessment\",\"pathology\"]\n",
    "new_mass_df = mass_df[cols_to_be_used]\n",
    "new_mass_df.rename(columns={\"breast_density\":\"breast density\"},inplace=True)\n",
    "\n",
    "bc_df = pd.concat([new_calc_df,new_mass_df])\n",
    "lbl_encoder = LabelEncoder()\n",
    "bc_df['left or right breast'] = lbl_encoder.fit_transform(bc_df['left or right breast'])\n",
    "bc_df['image view'] = lbl_encoder.fit_transform(bc_df['image view'])\n",
    "bc_df['calc type'] = lbl_encoder.fit_transform(bc_df['calc type'])\n",
    "bc_df[\"calc distribution\"] = lbl_encoder.fit_transform(bc_df['left or right breast'])\n",
    "bc_df['pathology'] = lbl_encoder.fit_transform(bc_df['pathology'])\n",
    "bc_df['mass shape'] = lbl_encoder.fit_transform(bc_df['mass shape'])\n",
    "bc_df['mass margins'] = lbl_encoder.fit_transform(bc_df['mass margins'])\n",
    "\n",
    "bc_df[\"breast density\"] = bc_df[\"breast density\"].astype(float)\n",
    "bc_df[\"abnormality id\"] = bc_df[\"abnormality id\"].astype(float)\n",
    "bc_df[\"assessment\"] = bc_df[\"assessment\"].astype(float)\n",
    "bc_df[\"breast density\"].fillna(np.mean(bc_df[\"breast density\"]),inplace=True)\n",
    "cols = [\"breast density\",'left or right breast',\"image view\",\"abnormality id\",\"assessment\", 'calc type','calc distribution', 'mass shape', 'mass margins']\n",
    "scaler = StandardScaler()\n",
    "bc_df[cols] = scaler.fit_transform(bc_df[cols])\n",
    "\n",
    "x_train,y_train = bc_df[cols].iloc[:2160], bc_df[\"pathology\"].iloc[:2160]\n",
    "x_val,y_val = bc_df[cols].iloc[2160:], bc_df[\"pathology\"].iloc[2160:]\n",
    "df = pd.concat([c_test,m_test])\n",
    "\n",
    "\n",
    "cols_to_be_used = [\"breast_density\",\"left or right breast\",\"image view\",\"abnormality id\",\"mass shape\",\"mass margins\",\"assessment\",\"pathology\"]\n",
    "new_mass_df = mass_df[cols_to_be_used]\n",
    "new_mass_df.rename(columns={\"breast_density\":\"breast density\"},inplace=True)\n",
    "\n",
    "\n",
    "df['left or right breast'] = lbl_encoder.fit_transform(df['left or right breast'])\n",
    "df['image view'] = lbl_encoder.fit_transform(df['image view'])\n",
    "df['calc type'] = lbl_encoder.fit_transform(df['calc type'])\n",
    "df[\"calc distribution\"] = lbl_encoder.fit_transform(df['left or right breast'])\n",
    "df['pathology'] = lbl_encoder.fit_transform(df['pathology'])\n",
    "df['mass shape'] = lbl_encoder.fit_transform(df['mass shape'])\n",
    "df['mass margins'] = lbl_encoder.fit_transform(df['mass margins'])\n",
    "\n",
    "df[\"breast density\"] = df[\"breast density\"].astype(float)\n",
    "df[\"abnormality id\"] = df[\"abnormality id\"].astype(float)\n",
    "df[\"assessment\"] = df[\"assessment\"].astype(float)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[cols] = scaler.fit_transform(df[cols])\n",
    "df[\"breast density\"].fillna(np.mean(df[\"breast density\"]),inplace=True)\n",
    "\n",
    "df = df[[\"breast density\",\"left or right breast\",\"image view\",\"abnormality id\",\"assessment\",\"calc type\",\"calc distribution\",\"pathology\",\"mass shape\",\"mass margins\"]] \n",
    "\n",
    "x_test,y_test = df[cols], df[\"pathology\"]\n",
    "\n",
    "class NumericalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16), # bu output degerini direkt olarak ensembe modele yollayacagiz cunku\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear_relu_stack(x)\n",
    "        return output\n",
    "\n",
    "y_train = np.reshape(y_train,(y_train.shape[0],1))\n",
    "y_val = np.reshape(y_val,(y_val.shape[0],1))\n",
    "y_test = np.reshape(y_test,(y_test.shape[0],1))\n",
    "\n",
    "\n",
    "train_set = np.concatenate((x_train,y_train),axis=1)\n",
    "val_set = np.concatenate((x_val,y_val),axis=1)\n",
    "test_set = np.concatenate((x_test,y_test),axis=1)\n",
    "train_set = torch.tensor(train_set, dtype=torch.float32)\n",
    "val_set = torch.tensor(val_set, dtype=torch.float32)\n",
    "test_set = torch.tensor(test_set, dtype=torch.float32)\n",
    "\n",
    "# print(train_set.shape)\n",
    "# print(val_set.shape)\n",
    "# print(test_set.shape)\n",
    "\n",
    "class EnsembleModel(nn.Module): # This neural network merges the \n",
    "\n",
    "    def __init__(self,resnet_input,numerical_input):\n",
    "        super(EnsembleModel,self).__init__()\n",
    "        self.resnet_input = resnet_input\n",
    "        self.numerical_input = numerical_input\n",
    "        self.total_params = torch.cat((self.resnet_input,self.numerical_input),1) \n",
    "        self.classifier = nn.Linear(32,1)\n",
    "\n",
    "\n",
    "    def forward(self,param):\n",
    "        param =  self.total_params\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        param = param.to(device)\n",
    "        return F.sigmoid(self.classifier(param))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_predictions = [] # The outputs of predictions will be stored in this array\n",
    "numerical_predictions = [] # Numerical predictions will be stored in this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sched import scheduler\n",
    "\n",
    "def numerical_validation(network,val_dataloader,voutputs,vlabels,batch_size,step=0,epoch_num = 0, mode=\"validation\"):\n",
    "    model = NumericalClassifier()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    val_loader  = DataLoader(val_set,  batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for step_num,values in enumerate(val_loader):\n",
    "            if step_num != step:\n",
    "                continue\n",
    "            inputs,labels = values[:,:9],values[:,9] \n",
    "            inputs,labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            train_ensemble_model(voutputs,outputs,vlabels,batch_size,epoch_num,\"validation\")\n",
    "            print(\"validation Numerical classifierdayim\",step_num)\n",
    "            validation(network,val_dataloader,batch_size,step_num+1,epoch_num= epoch_num, mode=\"validation\")\n",
    "\n",
    "\n",
    "\n",
    "def validation(network,val_dataloader,batch_size,step=0,epoch_num = 0, mode=\"validation\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            if i != step:\n",
    "                continue\n",
    "            print(\"Validation resnetteyim\",i)\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "            \n",
    "            voutputs = torch.squeeze(network(vinputs))\n",
    "            new_vlabels = torch.tensor(vlabels, dtype=torch.float32).to(device)\n",
    "            numerical_validation(network,val_dataloader,voutputs,new_vlabels,batch_size,i,epoch_num,\"validation\")\n",
    "\n",
    "def train_numerical_classifier(resnet_input,network,loader,optimizer,batch_size,target,step=0, epoch_num = 0, mode = \"training\"):\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = build_optimizer(NumericalClassifier(),\"adam\",learning_rate=0.001)\n",
    "    model = NumericalClassifier()\n",
    "    model.to(device)\n",
    "    for batch_num, temp in enumerate(train_loader):\n",
    "        inputs,labels = temp[:,:9], temp[:,9]\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\n",
    "        if batch_num != step:\n",
    "            continue\n",
    "        optimizer.zero_grad()      \n",
    "        numerical_output = model(inputs)\n",
    "        optimizer.step()\n",
    "        train_ensemble_model(resnet_input,numerical_output,target,batch_size, epoch_num, mode)\n",
    "        print(\"Numerical classifierdayim\",batch_num)\n",
    "        if batch_num == 32:\n",
    "            break\n",
    "        train_epoch(network,loader,optimizer,batch_size,step=batch_num+1)\n",
    "        # Train ensemble model loss u geriye dondurdukten sonra bir sonraki batch e gecmeden tekrardan resnet train kismina gecilmesi lazim ki orada bir sonraki batche gecilebilsin. \n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "def train_epoch(network, loader, optimizer, batch_size,step = 0, epoch_num = 0, mode = \"training\"):\n",
    "    cumu_loss = 0\n",
    "    network = build_resnet()    \n",
    "    for batch_number, (data, target) in enumerate(loader): # batch batch butun datayi gezip loss degerlerini hesapliyorum\n",
    "        # ensembe isini bitirdikten sonra bir sonraki batche gecilmesi gerekiyor bastan degil yani\n",
    "        \n",
    "        if batch_number != step:\n",
    "            continue\n",
    "        print(\"Trainingde batch sayim\",batch_number)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        data,target = data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        network.to(device)\n",
    "        # ➡ Forward pass\n",
    "        prediction = network(data)\n",
    "        prediction = torch.squeeze(prediction)\n",
    "        new_target = torch.tensor(target, dtype=torch.float32)\n",
    "        optimizer.step()\n",
    "        train_numerical_classifier(prediction,network,loader,optimizer,batch_size,new_target,batch_number, epoch_num, mode) # batch number baslangicta 0 burda\n",
    "        if batch_number == 32:\n",
    "            break\n",
    "\n",
    "\n",
    "batch_size = 0\n",
    "def train(config=None):\n",
    "    global batch_size\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        train_dataloader,test_dataloader,val_dataloader = build_dataset(config.batch_size)\n",
    "        network = build_resnet()\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "        #scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=10)\n",
    "        es = EarlyStopping()\n",
    "        best_val_loss = np.inf\n",
    "        PATH = './cifar_net.pth'\n",
    "        batch_size = config.batch_size\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            network.train(True)\n",
    "            print(\"Epoch num is\", epoch)\n",
    "            train_epoch(network, train_dataloader, optimizer,batch_size,step=0,epoch_num=epoch,mode=\"training\")\n",
    "            print(\"Validationa gectim epoch sayim\",epoch)\n",
    "            validation(network,val_dataloader,batch_size,step=0,epoch_num=epoch, mode=\"validation\")\n",
    "\n",
    "\n",
    "resnet_predictions = []\n",
    "\n",
    "\n",
    "optimizer = build_optimizer(NumericalClassifier(),\"adam\",learning_rate=0.001)\n",
    "current_mode = \"training\"\n",
    "predictions = []\n",
    "targets = []\n",
    "epoch_val = 0   \n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "prediction_list = []\n",
    "target_list = []\n",
    "metrics = []\n",
    "val_metrics = []\n",
    "best_loss = np.inf\n",
    "\n",
    "def train_ensemble_model(resnet_input,numerical_input,target,batch_size, epoch_num=0, mode=\"training\"):\n",
    "    global epoch_val\n",
    "    global current_mode\n",
    "    global metrics\n",
    "    global val_metrics\n",
    "    global best_loss\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ensemble_model = EnsembleModel(resnet_input,numerical_input)\n",
    "    ensemble_model.to(device)\n",
    "    if mode == \"train\":\n",
    "        ensemble_model.train()\n",
    "    else: # in validation mode\n",
    "        ensemble_model.eval()\n",
    "    #print(\"Parametreler\",ensemble_model.total_params.shape)\n",
    "    i = 0\n",
    "    if current_mode != mode: # this condition states that if the epoch has finished\n",
    "\n",
    "        #y_true_tensor = torch.tensor(targets)\n",
    "        #y_pred_tensor = torch.tensor(predictions)\n",
    "        #print(y_pred_tensor)\n",
    "        #print(y_true_tensor)\n",
    "        #sys.exit()\n",
    "        if current_mode == \"training\":\n",
    "            # print(predictions)\n",
    "            # print(targets)\n",
    "            # sys.exit()\n",
    "            avg_loss = sum(loss_values) / len(loss_values)\n",
    "            metrics = np.array(metrics)\n",
    "            wandb.log({\"training_loss\": avg_loss, \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"training_accuracy\": np.mean(metrics[:,0]), \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"training_precision\": np.mean(metrics[:,1]), \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"training_recall\": np.mean(metrics[:,2]), \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"training_f1\": np.mean(metrics[:,3]), \"epoch\": epoch_val+1})\n",
    "      \n",
    "            loss_values.clear()\n",
    "            predictions.clear()\n",
    "            targets.clear()\n",
    "            prediction_list.clear()\n",
    "            target_list.clear()\n",
    "            metrics = metrics.tolist()\n",
    "            metrics.clear()\n",
    "\n",
    "        if current_mode == \"validation\":\n",
    "            #print(\"Kod buraya girebiliyor\")\n",
    "            val_metrics = np.array(val_metrics)\n",
    "            avg_val_loss = sum(val_loss_values) / len(val_loss_values)\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                torch.save(ensemble_model.state_dict(),\"best_model_params.pt\")\n",
    "            wandb.log({\"validation_loss\": avg_val_loss, \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"validation_accuracy\": np.mean(val_metrics[:,0]), \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"validation_precision\": np.mean(val_metrics[:,1]), \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"validation_recall\": np.mean(val_metrics[:,2]), \"epoch\": epoch_val+1})\n",
    "            wandb.log({\"validation_f1_score\": np.mean(val_metrics[:,3]), \"epoch\": epoch_val+1})\n",
    "            val_loss_values.clear()\n",
    "            predictions.clear()\n",
    "            targets.clear()\n",
    "            prediction_list.clear()\n",
    "            target_list.clear()\n",
    "            val_metrics = val_metrics.tolist()\n",
    "            val_metrics.clear()\n",
    "\n",
    "        current_mode = mode\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    prediction_prob = ensemble_model(ensemble_model.total_params) # Buradan sigmoid cikisi 0 ile 1 arasinda bir deger donecek\n",
    "    prediction_label = prediction_prob.round() # sigmoidden gelen verileri burada 0 ya da 1 e yuvarliyoruz thresholdu 0.5 kabul ederek\n",
    "    target = torch.reshape(target, (target.shape[0],1))\n",
    "    \n",
    "    targets.extend(target.cpu().numpy().astype(int))\n",
    "    for t in targets:\n",
    "        target_list.append(t.item())\n",
    "    loss_val = nn.BCELoss()(prediction_label,target) / batch_size\n",
    "    # print(\"Loss val is\", loss_val)\n",
    "    if mode == \"training\":\n",
    "        print(\"training batch\")\n",
    "        loss_values.append(loss_val.item())\n",
    "        predictions.extend(prediction_label.detach().cpu().numpy().astype(int))\n",
    "        for i in predictions:\n",
    "            prediction_list.append(i.item())\n",
    "\n",
    "        batch_acc = accuracy_score(prediction_list,target_list)\n",
    "        batch_preicision = precision_score(prediction_list,target_list)\n",
    "        batch_recall = recall_score(prediction_list,target_list)\n",
    "        batch_f1 = f1_score(prediction_list,target_list)\n",
    "\n",
    "\n",
    "        wandb.log({\"batch_loss\": loss_val.item(), \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"batch_accuracy\":batch_acc , \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"batch_precision\": batch_preicision, \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"batch_recall\": batch_recall, \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"batch_f1\": batch_f1, \"epoch\": epoch_val+1})\n",
    "        metrics.append([batch_acc,batch_preicision,batch_recall,batch_f1])\n",
    "        loss_val.backward(retain_graph=True)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions.clear()\n",
    "        targets.clear()\n",
    "        prediction_list.clear()\n",
    "        target_list.clear()\n",
    "    else: # mode == \"validation\"\n",
    "        val_loss_values.append(loss_val.item())\n",
    "        #print(val_loss_values)\n",
    "        predictions.extend(prediction_label.cpu().numpy())\n",
    "        for i in predictions:\n",
    "            prediction_list.append(i.item())\n",
    "        batch_val_acc = accuracy_score(prediction_list,target_list)\n",
    "        batch_val_preicision = precision_score(prediction_list,target_list)\n",
    "        batch_val_recall = recall_score(prediction_list,target_list)\n",
    "        batch_val_f1 = f1_score(prediction_list,target_list)\n",
    "        wandb.log({\"val_batch_loss\": loss_val.item(), \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"val_batch_accuracy\": batch_val_acc, \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"val_precision\": batch_val_preicision, \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"val_batch_recall\": batch_val_recall, \"epoch\": epoch_val+1})\n",
    "        wandb.log({\"val_batch_f1\": batch_val_f1, \"epoch\": epoch_val+1})\n",
    "        val_metrics.append([batch_val_acc,batch_val_preicision,batch_val_recall,batch_val_f1])\n",
    "        # bu islemden sonra batch bitti ve cikmasi lazim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network = build_resnet()\n",
    "    best_checkpoint = torch.load('./best_model_params.pth')\n",
    "\n",
    "    network.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_dataloader,test_dataloader,val_dataloader = build_dataset(64)\n",
    "    for epoch in range(5):\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                images = images.to(device)\n",
    "                outputs = network(images)\n",
    "                pred_prob = torch.sigmoid(outputs)\n",
    "                predicted = pred_prob.round()\n",
    "                #_, predicted = torch.max(outputs, 1)\n",
    "                resnet_predictions.extend(outputs.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy()) # bu benim ust mlp ye koyacagim input oluyor su an\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "            \n",
    "            \n",
    "            test_acc = accuracy_score(y_pred,y_true)\n",
    "            test_preicision = precision_score(y_pred,y_true)\n",
    "            test_recall = recall_score(y_pred,y_true)\n",
    "            test_f1 = f1_score(y_pred,y_true)\n",
    "\n",
    "\n",
    "            print(f'Accuracy: {test_acc}')\n",
    "            print(f'Precision: {test_preicision}')\n",
    "            print(f'Recall: {test_recall}')\n",
    "            print(f'F1 Score: {test_f1}')\n",
    "            \n",
    "            wandb.log({\"test precision\":test_preicision})\n",
    "            wandb.log({\"test recall\":test_recall})\n",
    "            wandb.log({\"test f1 score\":test_f1})\n",
    "            wandb.log({\"test accuracy\":test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'values': [64]},\n",
      "                'epochs': {'values': [6]},\n",
      "                'learning_rate': {'values': [0.0001]},\n",
      "                'optimizer': {'values': ['adam']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nwnrq8x9\n",
      "Sweep URL: https://wandb.ai/alphanhilal-university-of-sussex/Pretrained%20Output%201/sweeps/nwnrq8x9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam']\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        \"values\" : [0.0001] # 0.0001 vermeyi bi dene bakalim\n",
    "      },\n",
    "      'batch_size': {\n",
    "        \"values\" : [64]\n",
    "      },\n",
    "      \"epochs\" : {\n",
    "        \"values\" : [6]\n",
    "      }\n",
    "    }\n",
    " \n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Pretrained Output 1\")\n",
    "\n",
    "wandb.agent(sweep_id, train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
