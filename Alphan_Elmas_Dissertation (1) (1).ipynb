{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbG69QSD28qR"
   },
   "source": [
    "# Finding the MRI brain tumor detection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcMFxKfv28qS"
   },
   "source": [
    "Let's find the dataset in this link: https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wsrtTwy28qS"
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch==2.0+cu117 --user -f https://download.pytorch.org/whl/cu117/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "yAZXXlHp28qS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "import cv2\n",
    "import sys\n",
    "from torchvision import models,transforms,datasets\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import torchvision\n",
    "from sklearn .metrics import roc_auc_score\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJtcehDm4H4Z",
    "outputId": "64b5758f-3074-4d9a-9f75-50ffc024f9d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A4000'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available()) # this should return true\n",
    "torch.cuda.get_device_name() # this should return your graphics card name. Ex) 'NVIDIA RTX A4000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "hV1jmdLT5LKF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom zipfile import ZipFile \\n\\n # loading the temp.zip and creating a zip object \\nwith ZipFile(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis_ddsm.zip\", \\'r\\') as zObject: \\n    zObject.extractall() \\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # importing the zipfile module \n",
    "\"\"\"\n",
    "from zipfile import ZipFile \n",
    "\n",
    " # loading the temp.zip and creating a zip object \n",
    "with ZipFile(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis_ddsm.zip\", 'r') as zObject: \n",
    "    zObject.extractall() \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.248386...\n",
       "2     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.267213...\n",
       "11    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.210396...\n",
       "12    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.749566...\n",
       "15    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.987658...\n",
       "Name: image_path, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dicom_data = pd.read_csv(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\dicom_info.csv\")\n",
    "\n",
    "full_mammogram_images = dicom_data[dicom_data.SeriesDescription == 'full mammogram images'].image_path\n",
    "full_mammogram_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# load the mass dataset\\nmass_train = pd.read_csv(\\'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_train_set.csv\\')\\nmass_test = pd.read_csv(\\'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_test_set.csv\\')\\n\\n#mass_train.head()\\nmass_train.iloc[0][11].split(\"/\")[2]\\n\\n# fix image paths\\ndef fix_image_path(data):\\n #   correct dicom paths to correct image paths\\n  for index, img in enumerate(data.values):\\n    img_name = img[11].split(\"/\")[2]\\n    new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\jpeg\\\\\" + img_name\\n    data.iloc[index,11] = new_path\\n      \\n# apply to datasets\\nfix_image_path(mass_train)\\nfix_image_path(mass_test)\\n'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# load the mass dataset\n",
    "mass_train = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_train_set.csv')\n",
    "mass_test = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\mass_case_description_test_set.csv')\n",
    "\n",
    "#mass_train.head()\n",
    "mass_train.iloc[0][11].split(\"/\")[2]\n",
    "\n",
    "# fix image paths\n",
    "def fix_image_path(data):\n",
    " #   correct dicom paths to correct image paths\n",
    "  for index, img in enumerate(data.values):\n",
    "    img_name = img[11].split(\"/\")[2]\n",
    "    new_path = \"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\jpeg\\\\\" + img_name\n",
    "    data.iloc[index,11] = new_path\n",
    "      \n",
    "# apply to datasets\n",
    "fix_image_path(mass_train)\n",
    "fix_image_path(mass_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncalc_train = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_train_set.csv')\\ncalc_test = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_test_set.csv')\\n\\nfix_image_path(calc_train)\\n\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "calc_train = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_train_set.csv')\n",
    "calc_test = pd.read_csv('C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\csv\\\\calc_case_description_test_set.csv')\n",
    "\n",
    "fix_image_path(calc_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntemp_calc_train = calc_train[[\"image file path\",\"pathology\"]]\\ncalc_normal = []\\ncalc_malignant = []\\nfor row,col in temp_calc_train.iterrows():\\n    if col[\"pathology\"] == \"MALIGNANT\":\\n        calc_malignant.append(col[\"image file path\"])\\n    else:\\n        calc_normal.append(col[\"image file path\"])\\n\\nprint(len(calc_normal))\\nprint(len(calc_malignant))\\nprint(len(temp_calc_train))\\n'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "temp_calc_train = calc_train[[\"image file path\",\"pathology\"]]\n",
    "calc_normal = []\n",
    "calc_malignant = []\n",
    "for row,col in temp_calc_train.iterrows():\n",
    "    if col[\"pathology\"] == \"MALIGNANT\":\n",
    "        calc_malignant.append(col[\"image file path\"])\n",
    "    else:\n",
    "        calc_normal.append(col[\"image file path\"])\n",
    "\n",
    "print(len(calc_normal))\n",
    "print(len(calc_malignant))\n",
    "print(len(temp_calc_train))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnew_df = mass_train[[\"image file path\",\"pathology\"]]\\nnew_df\\n\\nnormal_path = []\\nmalignant_path = []\\nfor row,col in new_df.iterrows():\\n    if col[\"pathology\"] == \"MALIGNANT\":\\n        # put this file to the malignant folder\\n        malignant_path.append(col[\"image file path\"])\\n    else:\\n        normal_path.append(col[\"image file path\"])\\n\\nprint(len(normal_path))\\nprint(len(malignant_path))\\nprint(len(new_df))\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "new_df = mass_train[[\"image file path\",\"pathology\"]]\n",
    "new_df\n",
    "\n",
    "normal_path = []\n",
    "malignant_path = []\n",
    "for row,col in new_df.iterrows():\n",
    "    if col[\"pathology\"] == \"MALIGNANT\":\n",
    "        # put this file to the malignant folder\n",
    "        malignant_path.append(col[\"image file path\"])\n",
    "    else:\n",
    "        normal_path.append(col[\"image file path\"])\n",
    "\n",
    "print(len(normal_path))\n",
    "print(len(malignant_path))\n",
    "print(len(new_df))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import train_test_split\\n\\nlast_df = pd.concat([temp_calc_train,new_df])\\ntemp,test = train_test_split(last_df, test_size=0.1,shuffle= True)\\ntrain,val = train_test_split(temp, test_size=0.1,shuffle= True)\\n\\nprint(len(train))\\nprint(len(val))\\nprint(len(test))\\n'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "last_df = pd.concat([temp_calc_train,new_df])\n",
    "temp,test = train_test_split(last_df, test_size=0.1,shuffle= True)\n",
    "train,val = train_test_split(temp, test_size=0.1,shuffle= True)\n",
    "\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport shutil\\n\\ndef write_into_train_folder():\\n    for row,col in train.iterrows():\\n        for img_name in os.listdir(col[\"image file path\"]):\\n            img_path = col[\"image file path\"] + \"\\\\\" + img_name\\n            if col[\"pathology\"] == \"MALIGNANT\":\\n                # put this file to the malignant folder\\n                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\malignant\")\\n            else:\\n                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\normal\")\\n                # put it into normal folder\\n\\n\\ndef write_into_val_folder():\\n    for row,col in val.iterrows():\\n        for img_name in os.listdir(col[\"image file path\"]):\\n            img_path = col[\"image file path\"] + \"\\\\\" + img_name\\n            if col[\"pathology\"] == \"MALIGNANT\":\\n                # put this file to the malignant folder\\n                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\malignant\")\\n            else:\\n                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\normal\")\\n                # put it into normal folder\\n\\n\\ndef write_into_test_folder():\\n    for row,col in test.iterrows():\\n        for img_name in os.listdir(col[\"image file path\"]):\\n            img_path = col[\"image file path\"] + \"\\\\\" + img_name\\n            if col[\"pathology\"] == \"MALIGNANT\":\\n                # put this file to the malignant folder\\n                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\malignant\")\\n            else:\\n                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\normal\")\\n                # put it into normal folder\\n\\nwrite_into_train_folder()\\nwrite_into_val_folder()\\nwrite_into_test_folder()\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import shutil\n",
    "\n",
    "def write_into_train_folder():\n",
    "    for row,col in train.iterrows():\n",
    "        for img_name in os.listdir(col[\"image file path\"]):\n",
    "            img_path = col[\"image file path\"] + \"\\\\\" + img_name\n",
    "            if col[\"pathology\"] == \"MALIGNANT\":\n",
    "                # put this file to the malignant folder\n",
    "                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\malignant\")\n",
    "            else:\n",
    "                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\normal\")\n",
    "                # put it into normal folder\n",
    "\n",
    "\n",
    "def write_into_val_folder():\n",
    "    for row,col in val.iterrows():\n",
    "        for img_name in os.listdir(col[\"image file path\"]):\n",
    "            img_path = col[\"image file path\"] + \"\\\\\" + img_name\n",
    "            if col[\"pathology\"] == \"MALIGNANT\":\n",
    "                # put this file to the malignant folder\n",
    "                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\malignant\")\n",
    "            else:\n",
    "                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val\\\\normal\")\n",
    "                # put it into normal folder\n",
    "\n",
    "\n",
    "def write_into_test_folder():\n",
    "    for row,col in test.iterrows():\n",
    "        for img_name in os.listdir(col[\"image file path\"]):\n",
    "            img_path = col[\"image file path\"] + \"\\\\\" + img_name\n",
    "            if col[\"pathology\"] == \"MALIGNANT\":\n",
    "                # put this file to the malignant folder\n",
    "                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\malignant\")\n",
    "            else:\n",
    "                shutil.copy(img_path,\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test\\\\normal\")\n",
    "                # put it into normal folder\n",
    "\n",
    "write_into_train_folder()\n",
    "write_into_val_folder()\n",
    "write_into_test_folder()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os \\n#os.makedirs(\"cbis-train/normal\")\\nos.makedirs(\"cbis-train/malignant\")\\nos.makedirs(\"cbis-val/normal\")\\nos.makedirs(\"cbis-val/malignant\")\\nos.makedirs(\"cbis-test/normal\")\\nos.makedirs(\"cbis-test/malignant\")\\n\\ndef write_inside_folder(df):\\n    for folder in df:\\n        for file in os.listdir(folder):\\n            cv2.imwrite(folder,file)\\n\\n#write_inside_folder(train)\\n#write_inside_folder(val)\\n#write_inside_folder(test)\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os \n",
    "#os.makedirs(\"cbis-train/normal\")\n",
    "os.makedirs(\"cbis-train/malignant\")\n",
    "os.makedirs(\"cbis-val/normal\")\n",
    "os.makedirs(\"cbis-val/malignant\")\n",
    "os.makedirs(\"cbis-test/normal\")\n",
    "os.makedirs(\"cbis-test/malignant\")\n",
    "\n",
    "def write_inside_folder(df):\n",
    "    for folder in df:\n",
    "        for file in os.listdir(folder):\n",
    "            cv2.imwrite(folder,file)\n",
    "\n",
    "#write_inside_folder(train)\n",
    "#write_inside_folder(val)\n",
    "#write_inside_folder(test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "#run = wandb.init(\n",
    " #   # Set the project where this run will be logged\n",
    "  #  project=\"tezim\",\n",
    "   # # Track hyperparameters and run metadata\n",
    "#)\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHE():\n",
    "    \n",
    "    def __init__(self, clipLimit=2.0, tileGridSize=(8,8)):\n",
    "        super().__init__\n",
    "        self.clipLimit = clipLimit\n",
    "        self.tileGridSize = tileGridSize\n",
    "\n",
    "    def __call__(self,sample):\n",
    "      img = np.array(sample)\n",
    "      img = np.moveaxis(img,0,2)\n",
    "      imgray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "      imgray = imgray.astype(np.uint16)\n",
    "      clahe = cv2.createCLAHE(clipLimit=self.clipLimit, tileGridSize=self.tileGridSize)\n",
    "      new_img = clahe.apply(imgray)\n",
    "      new_img = cv2.cvtColor(new_img, cv2.COLOR_GRAY2BGR)\n",
    "      new_img = cv2.cvtColor(new_img, cv2.COLOR_BGR2RGB)\n",
    "      new_img = new_img.astype(np.float32)\n",
    "      new_img = np.moveaxis(new_img,2,0)\n",
    "      tensor = torch.tensor(new_img)\n",
    "      #print(img.shape)\n",
    "      return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2800, 3, 4808)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train\\\\malignant\\\\1-000.jpg\")\n",
    "print(type(img))\n",
    "img = np.moveaxis(img,0,2)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size):\n",
    "   \n",
    "    transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), # data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # normalization\n",
    "    CLAHE()\n",
    "\n",
    "])\n",
    "    \n",
    "    train_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train'\n",
    "    #data = BCDataset(train_dir,transforms_train)\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transforms_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    test_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test'\n",
    "    #data_test = BCDataset(test_dir,transforms_train)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transforms_train)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    val_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val'\n",
    "    #data_val = BCDataset(val_dir,transforms_train)\n",
    "    val_dataset = datasets.ImageFolder(val_dir, transforms_train)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_dataloader,test_dataloader,val_dataloader\n",
    "\n",
    "\n",
    "def build_resnet():\n",
    "    model_ft = models.resnet50(weights='IMAGENET1K_V1')\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return model_ft.to(device)\n",
    "\n",
    "def build_densenet():\n",
    "    model_ft = models.densenet169(weights='IMAGENET1K_V1')\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return model_ft.to(device)\n",
    "\n",
    "def build_vggnet():\n",
    "    model_ft = models.vgg19(weights='IMAGENET1K_V1')\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return model_ft.to(device)\n",
    "\n",
    "\n",
    "def build_inception():\n",
    "    model = models.inception_v3(pretrained=True)\n",
    "    model.aux_logits = False\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 10),\n",
    "    nn.Linear(10, 2)\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def inception_dataset(batch_size):\n",
    "    train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(299),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # ToTensor : [0, 255] -> [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "    \n",
    "    train_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-train'\n",
    "    train_dataset = datasets.ImageFolder(train_dir, train_transform)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    test_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-test'\n",
    "    test_dataset = datasets.ImageFolder(test_dir, train_transform)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    val_dir = 'C:\\\\Users\\\\ae423\\\\OneDrive - University of Sussex\\\\Desktop\\\\AlphanElmasDissertation\\\\cbis-val'\n",
    "    val_dataset = datasets.ImageFolder(val_dir, train_transform)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_dataloader,test_dataloader,val_dataloader\n",
    "        \n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "    cumu_loss = 0\n",
    "    for _, (data, target) in enumerate(loader): # batch batch butun datayi gezip loss degerlerini hesapliyorum\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ➡ Forward pass\n",
    "        loss = nn.CrossEntropyLoss()(network(data), target)\n",
    "        cumu_loss += loss.item()\n",
    "\n",
    "        # ⬅ Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"batch loss\": loss.item()})\n",
    "\n",
    "    return cumu_loss / len(loader)\n",
    "\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        train_dataloader,test_dataloader,val_dataloader = build_dataset(config.batch_size)\n",
    "        network = build_resnet()\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            network.train(True)\n",
    "            avg_loss = train_epoch(network, train_dataloader, optimizer)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})    \n",
    "            running_vloss = 0\n",
    "            network.eval()\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            with torch.no_grad():\n",
    "                for i, vdata in enumerate(val_dataloader):\n",
    "                    vinputs, vlabels = vdata\n",
    "                    vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
    "                    voutputs = network(vinputs)\n",
    "                    vloss = loss_fn(voutputs, vlabels)\n",
    "                    running_vloss += vloss     \n",
    "\n",
    "                avg_valloss = running_vloss / (i + 1)\n",
    "                wandb.log({\"validation_loss\":avg_valloss, \"epoch\": epoch})\n",
    "        \n",
    "        PATH = './cifar_net.pth'\n",
    "        torch.save(network.state_dict(), PATH)\n",
    "\n",
    "\n",
    "def test():\n",
    "    PATH = './cifar_net.pth'\n",
    "    network = build_resnet()\n",
    "    network.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    network.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_dataloader,test_dataloader,val_dataloader = build_dataset(32)\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = network(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "\n",
    "    # Convert lists to tensors for calculation\n",
    "    y_true_tensor = torch.tensor(y_true)\n",
    "    y_pred_tensor = torch.tensor(y_pred)\n",
    "\n",
    "    # Calculating precision, recall, and F1 score using PyTorch\n",
    "    TP = ((y_pred_tensor == 1) & (y_true_tensor == 1)).sum().item()\n",
    "    FP = ((y_pred_tensor == 1) & (y_true_tensor == 0)).sum().item()\n",
    "    FN = ((y_pred_tensor == 0) & (y_true_tensor == 1)).sum().item()\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    auc = np.round(roc_auc_score(y_true, y_pred), 3)\n",
    "\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    \n",
    "    wandb.log({\"precision\":precision})\n",
    "    wandb.log({\"recall\":recall})\n",
    "    wandb.log({\"f1 score\":f1})\n",
    "    wandb.log({\"AUC\":auc})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'values': [32, 64, 128, 256]},\n",
      "                'epochs': {'values': [10, 20]},\n",
      "                'learning_rate': {'values': [0.001, 0.0001]},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n",
      "Create sweep with ID: ucuu6d1z\n",
      "Sweep URL: https://wandb.ai/alphanhilal-university-of-sussex/tezim/sweeps/ucuu6d1z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: fjesvdgp with config:\n",
      "wandb: \tbatch_size: 32\n",
      "wandb: \tepochs: 10\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\ae423\\OneDrive - University of Sussex\\Desktop\\AlphanElmasDissertation\\wandb\\run-20240820_181528-fjesvdgp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alphanhilal-university-of-sussex/tezim/runs/fjesvdgp' target=\"_blank\">earthy-sweep-1</a></strong> to <a href='https://wandb.ai/alphanhilal-university-of-sussex/tezim' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/alphanhilal-university-of-sussex/tezim/sweeps/ucuu6d1z' target=\"_blank\">https://wandb.ai/alphanhilal-university-of-sussex/tezim/sweeps/ucuu6d1z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alphanhilal-university-of-sussex/tezim' target=\"_blank\">https://wandb.ai/alphanhilal-university-of-sussex/tezim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/alphanhilal-university-of-sussex/tezim/sweeps/ucuu6d1z' target=\"_blank\">https://wandb.ai/alphanhilal-university-of-sussex/tezim/sweeps/ucuu6d1z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alphanhilal-university-of-sussex/tezim/runs/fjesvdgp' target=\"_blank\">https://wandb.ai/alphanhilal-university-of-sussex/tezim/runs/fjesvdgp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        \"values\" : [0.001, 0.0001]\n",
    "      },\n",
    "      'batch_size': {\n",
    "        \"values\" : [32,64,128,256]\n",
    "      },\n",
    "      \"epochs\" : {\n",
    "        \"values\" : [10,20]\n",
    "      }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"tezim\")\n",
    "\n",
    "wandb.agent(sweep_id, train)\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
